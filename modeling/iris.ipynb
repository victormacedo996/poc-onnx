{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook para modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=\"INFO\", stream=sys.stdout)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  target\n",
       "0           5.1          3.5           1.4          0.2     0.0\n",
       "1           4.9          3.0           1.4          0.2     0.0\n",
       "2           4.7          3.2           1.3          0.2     0.0\n",
       "3           4.6          3.1           1.5          0.2     0.0\n",
       "4           5.0          3.6           1.4          0.2     0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_iris(as_frame=True)\n",
    "df_dataset = data1 = pd.DataFrame(data= np.c_[dataset['data'], dataset['target']],\n",
    "                     columns= dataset['feature_names'] + ['target'])\n",
    "\n",
    "df_dataset.rename(columns={\n",
    "    \"sepal length (cm)\": \"sepal_length\",\n",
    "    \"sepal width (cm)\": \"sepal_width\",\n",
    "    \"petal length (cm)\": \"petal_length\",\n",
    "    \"petal width (cm)\": \"petal_width\",\n",
    "}, inplace=True)\n",
    "df_dataset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target\n",
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df_dataset[[\"target\"]]\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_dataset[[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]]\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, \n",
    "    target, \n",
    "    stratify=target, \n",
    "    test_size=0.2, \n",
    "    random_state=14\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo o pipeline do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A criação do pipeline tem três etapas:\n",
    "1. Criação do inputer para lidar com valores vazios. Dessa forma, se uma das features tiver um valor vazio, ele será completado com a média dos outros dois valores;\n",
    "2. Adição de um StandardScaler para normalização dos dados;\n",
    "3. O estimador, ou modelo, de fato que fará a predição. Para esse exemplo, foi escolhido uma regressão logistica, dada a simplicidade e agilidade no treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps = [\n",
    "        ('Imputer', SimpleImputer(strategy='mean', keep_empty_features=True)),\n",
    "        ('normalization', StandardScaler()),\n",
    "        ('estimator', LogisticRegression())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo o treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma a encontrar o melhor modelo possível, foi utilizada a técnica de `GridSearchParam` utilizando os possíveis parametros descritos abaixo. Além disso, foi realizada a validação cruzada utilizando cinco folds e a métrica alvo foi definida como AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "        'estimator__solver': ['newton-cg'],\n",
    "        'estimator__tol': [ 0.0001, 0.003, 0.01],\n",
    "        'estimator__penalty': [None, 'l2'],\n",
    "    }\n",
    "\n",
    "model = GridSearchCV(estimator=pipeline,\n",
    "                            param_grid=parameters,\n",
    "                            scoring= {\"AUC\": \"roc_auc_ovr\"},\n",
    "                            refit=\"AUC\",\n",
    "                            cv=5,\n",
    "                            verbose=1,\n",
    "                            error_score='raise')\n",
    "\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro fazemos a predição do dataset de teste utilizando o modelo previamente treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez tendo realizado as predições no dataset de teste, podemos calcular métricas de performance de modelo, como acurácia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Accuracy test score: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "test_acc_score = accuracy_score(y_test, y_pred)\n",
    "logging.info(f\"Accuracy test score: {test_acc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez estando satisfeito com o resultado do modelo, podemos persistir esse modelo como um arquivo pickle, para posteriormente só carregarmos esse modelo e usarmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Model saved in /mnt/3c2f822b-db13-4837-ba6e-3d7b256042cc/repositorios/poc-onnx/modeling/../models/logistic_regression_iris.pkl\n"
     ]
    }
   ],
   "source": [
    "current_path = os.path.abspath('')\n",
    "save_model_path = f\"{current_path}/../models/logistic_regression_iris.pkl\"\n",
    "\n",
    "with open(save_model_path, \"wb\") as pickle_file:\n",
    "    pickle.dump(model, pickle_file)\n",
    "\n",
    "logging.info(f\"Model saved in {save_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo para OONX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import das libs para conversão de scikit-learn para ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o modelo previamente treinado através do arquivo pickle salvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise',\n",
      "             estimator=Pipeline(steps=[('Imputer',\n",
      "                                        SimpleImputer(keep_empty_features=True)),\n",
      "                                       ('normalization', StandardScaler()),\n",
      "                                       ('estimator', LogisticRegression())]),\n",
      "             param_grid={'estimator__penalty': [None, 'l2'],\n",
      "                         'estimator__solver': ['newton-cg'],\n",
      "                         'estimator__tol': [0.0001, 0.003, 0.01]},\n",
      "             refit='AUC', scoring={'AUC': 'roc_auc_ovr'}, verbose=1)\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{current_path}/../models/logistic_regression_iris.pkl\", 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que ele é do tipo `GridSearchCV`. Dessa forma, devemos utilizar para conversão o melhor estimador gerado pelo `GridSearchCV`. Podemos ter acesso a ele através do atributo `best_estimator_`.\n",
    "\n",
    "Além disso, também devemos definir o tipo de input que nosso modelo ONNX irá receber, seguindo o mesmo formato utilizado na definição do nosso modelo durante o treinamento. No nosso caso, temos um modelo que recebe 4 inputs no formato float (sepal_length, sepal_width, petal_length, petal_width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_type = [('float_input', FloatTensorType([None, 4]))]\n",
    "onnx_model = convert_sklearn(model.best_estimator_, initial_types=initial_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez com nosso modelo convertido, podemos salvá-lo, igual fizemos com o modelo no formato pickle. Porém, dessa vez, iremos salvá-lo no formato onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{current_path}/../models/logistic_regression_iris.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o modelo ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import das libs para carregar um modelo ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Através do onnx runtime, podemos fazer o carregamento do nosso modelo salvo previamente e garantir que carregamos o modelo certo ao verificar o nome do input que demos na etapa de salvamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Name: float_input\n"
     ]
    }
   ],
   "source": [
    "sess = rt.InferenceSession(f\"{current_path}/../models/logistic_regression_iris.onnx\")\n",
    "input_name = sess.get_inputs()[0].name\n",
    "print(\"Input Name:\", input_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que modelos ONNX, em ambiente Python, trabalham com dados no formato numpy, iremos converter o dataset de teste para numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_np = X_test.astype(np.float32).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez com os dados prontos para serem utilizados, podemos utilizar o nosso objetivo `InferenceSession` para realizar a inferencia utilizando o input que criamos para o modelo e os dados convertidos para numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = sess.run(None, {input_name: X_test_np})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare que a predição nos retornou tanto a classe prevista quanto a distribuição de probabilidade para cada classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [array([2, 1, 2, 0, 1, 0, 0, 2, 1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 1, 1, 2, 2,\n",
      "       1, 2, 0, 2, 2, 0, 0, 0], dtype=int64), [{0: 4.3682564864866436e-05, 1: 0.18593932688236237, 2: 0.8140169978141785}, {0: 0.00045449292520061135, 1: 0.9904571175575256, 2: 0.00908839050680399}, {0: 1.0313229437119986e-11, 1: 0.0020974131766706705, 2: 0.9979026317596436}, {0: 0.9978068470954895, 1: 0.0021931955125182867, 2: 3.0318691893009564e-13}, {0: 0.0001320976298302412, 1: 0.990864098072052, 2: 0.00900376308709383}, {0: 0.997782289981842, 1: 0.002217723987996578, 2: 1.130992977223244e-12}, {0: 0.9995378255844116, 1: 0.0004621815460268408, 2: 4.608343871434495e-13}, {0: 8.596723333198497e-09, 1: 0.0024428837932646275, 2: 0.9975571036338806}, {0: 0.0006489232182502747, 1: 0.9930737018585205, 2: 0.0062774126417934895}, {0: 0.0017745760269463062, 1: 0.9635270237922668, 2: 0.03469844162464142}, {0: 0.9987296462059021, 1: 0.0012703877873718739, 2: 2.3457171535040677e-12}, {0: 1.2049142242176458e-05, 1: 0.9357765316963196, 2: 0.0642114207148552}, {0: 6.34794261600291e-08, 1: 0.10741561651229858, 2: 0.8925843238830566}, {0: 0.0019067322136834264, 1: 0.9968459010124207, 2: 0.001247346750460565}, {0: 0.9945306181907654, 1: 0.005469323601573706, 2: 3.919251641976151e-12}, {0: 0.9999198913574219, 1: 8.008448639884591e-05, 2: 8.85684331186935e-15}, {0: 0.00046304750139825046, 1: 0.9739784002304077, 2: 0.025558559224009514}, {0: 0.0005382936797104776, 1: 0.9640409350395203, 2: 0.03542081266641617}, {0: 2.3759812393109314e-05, 1: 0.6814839243888855, 2: 0.3184923529624939}, {0: 0.0018318488728255033, 1: 0.9755007028579712, 2: 0.02266746200621128}, {0: 1.7017765685523045e-07, 1: 0.03675802797079086, 2: 0.9632418155670166}, {0: 4.429679734130332e-09, 1: 0.002039702143520117, 2: 0.9979603290557861}, {0: 0.00026910967426374555, 1: 0.938925564289093, 2: 0.06080533564090729}, {0: 7.872704941291886e-07, 1: 0.056688833981752396, 2: 0.9433103799819946}, {0: 0.9980121850967407, 1: 0.001987815834581852, 2: 3.4519010913558468e-12}, {0: 1.6185076674446464e-06, 1: 0.12568655610084534, 2: 0.8743118643760681}, {0: 3.58780276599191e-08, 1: 0.01588837429881096, 2: 0.9841115474700928}, {0: 0.9991305470466614, 1: 0.0008694921270944178, 2: 2.0110155945010644e-13}, {0: 0.9964582920074463, 1: 0.003541700541973114, 2: 4.644397019748503e-11}, {0: 0.9959965944290161, 1: 0.0040033780969679356, 2: 1.3028080133800635e-12}]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos conferir se o output do modelo convertido é igual ao modelo antes da conversão de forma a garantir que os resultados são iguais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 2. 0. 1. 0. 0. 2. 1. 1. 0. 1. 2. 1. 0. 0. 1. 1. 1. 1. 2. 2. 1. 2.\n",
      " 0. 2. 2. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test_np)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
